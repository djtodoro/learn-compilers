\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{geometry}
\geometry{margin=1in}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=pythonstyle}

\title{Learn Compiler Design Through xDSL:\\
A Practical Course for AI/ML Engineers}
\author{Djordje Todorovic}
\date{August 2025}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Preface: Who This Course Is For and Why Compilers Matter}

\subsection{Prerequisites and Intended Audience}

This course is designed for engineers and developers who want to understand the fundamental concepts of compiler design through hands-on experience with xDSL. To get the most out of this material, you should have:

\begin{itemize}
    \item \textbf{Python Proficiency}: Strong knowledge of Python is essential, as xDSL is a Python-native framework. You should be comfortable with classes, decorators, type hints, and the Python standard library.
    \item \textbf{Basic Programming Knowledge}: Understanding of fundamental programming concepts like functions, variables, control flow, data structures, and algorithms is required.
    \item \textbf{Machine Learning and AI Basics}: Familiarity with ML/AI concepts is crucial - you should understand tensors, neural networks, computational graphs, and optimization techniques. Experience with frameworks like PyTorch or TensorFlow will help you appreciate the compiler optimizations we'll explore.
    \item \textbf{Mathematical Foundations}: Basic understanding of linear algebra and discrete mathematics will be helpful, especially when working with optimization algorithms and graph transformations.
\end{itemize}

If you're an ML engineer curious about what happens "under the hood" when your models are compiled and optimized, or a systems programmer interested in building efficient code transformation tools, this course will bridge that gap using familiar Python syntax.

\subsection{Compilers: The Invisible Revolution That Changed Computing}

Compilers are perhaps the most transformative technology in the history of computing, yet they work so seamlessly that we rarely think about them. To understand their revolutionary impact, consider this: the Linux kernel, which powers billions of devices worldwide, was initially written in assembly language - a tedious, error-prone process where programmers had to think in terms of individual CPU instructions and memory addresses. Each line of assembly code corresponded directly to a machine instruction, making even simple tasks require hundreds of lines of code.

Then came the evolution of the C compiler. As compilers matured, they could translate high-level C code into assembly with equal or sometimes even better performance than hand-written assembly. This transformation was revolutionary - Linux could be rewritten in C, making it portable across different architectures, easier to maintain, and accessible to a broader community of developers. What once required intimate knowledge of specific CPU architectures could now be expressed in readable, maintainable code. The compiler handled the complex translation, optimization, and architecture-specific details automatically.

This same revolution continues today in machine learning. Just as C compilers freed developers from assembly, ML compilers like XLA, TVM, and MLIR free ML engineers from writing architecture-specific kernels. Your PyTorch model can run efficiently on CPUs, GPUs, TPUs, or custom accelerators, all thanks to sophisticated compiler technology working behind the scenes.

\subsection{The Core Mission: Represent and Transform}

At its heart, a compiler's job is elegantly simple yet profoundly powerful: to \textbf{represent} programs in structured forms and to \textbf{transform} these representations to achieve specific goals. Think of a compiler as a sophisticated translator that not only converts between languages but also understands the meaning of what it's translating deeply enough to improve it along the way.

This representation and transformation paradigm involves:
\begin{itemize}
    \item \textbf{Representation}: Converting source code into structured data (Abstract Syntax Trees, Intermediate Representations, Control Flow Graphs) that capture the program's semantics precisely
    \item \textbf{Analysis}: Understanding data dependencies, control flow, memory access patterns, and optimization opportunities within these representations
    \item \textbf{Transformation}: Systematically modifying these representations to optimize performance, reduce resource usage, or target specific hardware architectures
    \item \textbf{Preservation}: Ensuring that transformations maintain the original program's correctness and semantics
\end{itemize}

\subsection{The Power of Serialize, Deserialize, and Verify}

A crucial but often overlooked aspect of compiler infrastructure is the ability to serialize, deserialize, and verify intermediate representations. This capability fundamentally changes how we can work with compilers and is a cornerstone of modern compiler design.

\textbf{Why These Capabilities Matter:}
\begin{itemize}
    \item \textbf{Serialization} allows us to save the compiler's intermediate state to disk, enabling separate compilation, caching of optimization results, and distribution of compiled modules. You can pause compilation, save the IR, and resume later - or on a different machine entirely.
    \item \textbf{Deserialization} lets us load previously compiled modules, compose them together, and build large systems incrementally. This is essential for modular compilation and linking.
    \item \textbf{Verification} ensures that the IR is well-formed and obeys the type system and semantic rules. This catches errors early, enables safe transformations, and provides guarantees about program behavior. Without verification, compiler bugs could silently corrupt programs.
\end{itemize}

The xDSL infrastructure we're about to explore has these capabilities built into its core. Every operation, every transformation, and every intermediate state can be serialized to a human-readable textual format, loaded back, and verified for correctness. This isn't just a convenience feature - it's fundamental to building robust, composable compiler pipelines. You can inspect the IR at any stage, debug transformations by examining the before-and-after states, and even hand-write IR for testing.

This approach contrasts sharply with traditional compilers where the intermediate states are often opaque binary structures locked inside the compiler's memory. With xDSL (inspired by MLIR), the entire compilation pipeline becomes transparent, debuggable, and extensible. You can serialize the IR after each optimization pass, analyze what changed, and even replay specific transformations. This visibility and control make compiler development more like software engineering and less like black magic.

As you progress through this course, you'll come to appreciate how these fundamental capabilities - serialize, deserialize, and verify - enable you to build reliable, maintainable, and powerful compilation tools. They transform the compiler from a monolithic black box into a modular, inspectable, and trustworthy system.

\section{Introduction: Compilers as Representation and Transformation Engines}

\subsection{Welcome to the World of Compilers}

If you're coming from an AI/ML background, you already understand the power of transforming data through layers of abstraction. Compilers are remarkably similar: they take programs written in one representation and systematically transform them into another, more optimized or more executable form.

Think of a compiler as a sophisticated pipeline that:
\begin{enumerate}
    \item \textbf{Represents} programs as structured data (Abstract Syntax Trees, Intermediate Representations)
    \item \textbf{Transforms} these representations through a series of optimization passes
    \item \textbf{Analyzes} code to understand dependencies, patterns, and optimization opportunities
    \item \textbf{Generates} efficient target code for specific hardware
\end{enumerate}

\subsection{Why xDSL?}

xDSL is a Python-native compiler framework that makes compiler concepts accessible to Python programmers. Unlike traditional compiler frameworks written in C++ (like LLVM), xDSL allows you to:
\begin{itemize}
    \item Build compilers using familiar Python syntax
    \item Prototype and experiment quickly
    \item Understand compiler internals without low-level complexity
    \item Leverage the entire Python ecosystem for analysis and visualization
\end{itemize}

\subsection{Course Philosophy: Learn by Building}

This course takes a hands-on approach. Instead of starting with theory, we'll build small compilers and optimization passes from day one. Each concept will be introduced through practical examples that you can run, modify, and experiment with.

\subsection{What You'll Learn}

By the end of this course, you'll understand:
\begin{itemize}
    \item How compilers represent programs internally (IR - Intermediate Representations)
    \item What operations and operators mean in a compiler context
    \item How to write analysis passes that extract information from code
    \item How to implement transformations that optimize programs
    \item The connection between compiler optimizations and ML model optimization
\end{itemize}

\section{Chapter 1: Understanding Intermediate Representations (IR)}

\subsection{What is an IR?}

An Intermediate Representation (IR) is the compiler's internal language for representing programs. Just as neural networks use tensors to represent data, compilers use IRs to represent code structure and semantics.

Key properties of IRs:
\begin{itemize}
    \item \textbf{Abstract}: Higher-level than machine code, but more structured than source code
    \item \textbf{Explicit}: Makes implicit operations visible (e.g., memory allocations, type conversions)
    \item \textbf{Analyzable}: Designed for easy pattern matching and transformation
    \item \textbf{Hierarchical}: Can represent nested structures (functions, loops, blocks)
\end{itemize}

\subsection{The Compiler Pipeline: From Source to Machine Code}

Before diving into xDSL specifics, let's understand how compilers work. A compiler is traditionally divided into three main phases, each with specific responsibilities:

\subsubsection{Frontend: Understanding Your Code}

The frontend translates source code into an intermediate representation. It consists of:

\textbf{1. Lexical Analysis (Lexer/Tokenizer)}
\begin{itemize}
    \item Breaks source code into tokens (keywords, identifiers, operators)
    \item Like splitting a sentence into words
    \item Example: \texttt{"x = 42 + y"} becomes tokens: [\texttt{ID:x}, \texttt{ASSIGN}, \texttt{NUM:42}, \texttt{PLUS}, \texttt{ID:y}]
\end{itemize}

\textbf{2. Syntax Analysis (Parser)}
\begin{itemize}
    \item Builds an Abstract Syntax Tree (AST) from tokens
    \item Checks grammatical structure
    \item Like diagramming a sentence to understand its structure
\end{itemize}

\begin{lstlisting}[language=Python, caption=Simple AST Example]
# Source: x = 42 + y
# AST representation:
Assignment(
    left=Variable("x"),
    right=BinaryOp(
        op="+",
        left=Number(42),
        right=Variable("y")
    )
)
\end{lstlisting}

\textbf{3. Semantic Analysis}
\begin{itemize}
    \item Type checking: Is this operation valid?
    \item Symbol resolution: What does this variable refer to?
    \item Example: Can't add a string to a number, variables must be declared
\end{itemize}

\subsubsection{Middle-end: Optimization Central}

The middle-end works on the IR to optimize code:
\begin{itemize}
    \item \textbf{Platform-independent}: Optimizations work for any target
    \item \textbf{Examples}: Dead code elimination, constant folding, loop optimization
    \item \textbf{Multiple passes}: Each optimization is a separate pass over the IR
\end{itemize}

\subsubsection{Backend: Generating Machine Code}

The backend translates optimized IR to target-specific code:
\begin{itemize}
    \item \textbf{Instruction selection}: Choose machine instructions
    \item \textbf{Register allocation}: Assign variables to CPU registers
    \item \textbf{Instruction scheduling}: Order operations for performance
\end{itemize}

\subsection{Learning from Giants: LLVM and MLIR}

\subsubsection{LLVM: The Industry Standard}

LLVM (Low Level Virtual Machine) is the most widely used compiler infrastructure:
\begin{itemize}
    \item Powers Clang (C/C++), Swift, Rust, and many other languages
    \item Provides a well-defined IR (LLVM IR) that many frontends target
    \item Excellent tutorial: \href{https://llvm.org/docs/tutorial/MyFirstLanguageFrontend/index.html}{Kaleidoscope Tutorial}
    \item Shows how to build a complete compiler for a simple language
\end{itemize}

\begin{lstlisting}[language=C, caption=LLVM IR Example]
; LLVM IR for: int add(int a, int b) { return a + b; }
define i32 @add(i32 %a, i32 %b) {
entry:
  %sum = add i32 %a, %b
  ret i32 %sum
}
\end{lstlisting}

\subsubsection{MLIR: Multi-Level IR}

MLIR (Multi-Level Intermediate Representation) extends LLVM's concepts:
\begin{itemize}
    \item Developed by Google, now part of LLVM project
    \item Supports multiple IRs (dialects) in the same infrastructure
    \item Used by TensorFlow, PyTorch, and other ML frameworks
    \item Tutorial: \href{https://mlir.llvm.org/docs/Tutorials/Toy/}{Toy Language Tutorial}
    \item xDSL is heavily inspired by MLIR but implemented in Python
\end{itemize}

\begin{lstlisting}[caption=MLIR Example - Multiple Abstraction Levels]
// High-level ML operation
%result = "tf.MatMul"(%a, %b) : (tensor<2x3xf32>, tensor<3x4xf32>) 
                                -> tensor<2x4xf32>

// After lowering to linalg dialect
%result = linalg.matmul ins(%a, %b : memref<2x3xf32>, memref<3x4xf32>)
                       outs(%c : memref<2x4xf32>)

// After lowering to loops
scf.for %i = 0 to 2 {
  scf.for %j = 0 to 4 {
    scf.for %k = 0 to 3 {
      // actual computation
    }
  }
}
\end{lstlisting}

\subsection{Why xDSL Uses IR Instead of AST}

While traditional compilers start with AST, xDSL (like MLIR) works directly with IR because:
\begin{itemize}
    \item \textbf{Uniformity}: All transformations work on the same structure
    \item \textbf{Composability}: Easy to combine different languages/dialects
    \item \textbf{Reusability}: Optimizations work across different source languages
    \item \textbf{Analysis-friendly}: SSA form makes many analyses trivial
\end{itemize}

Think of it this way:
\begin{itemize}
    \item \textbf{AST}: Like a sentence diagram - shows structure
    \item \textbf{IR}: Like assembly with types - shows computation
\end{itemize}

\subsection{SSA Form: Single Static Assignment}

xDSL uses SSA (Single Static Assignment) form, where each variable is assigned exactly once. This is similar to functional programming and makes many analyses simpler.

\begin{lstlisting}[language=Python, caption=Traditional vs SSA Form]
# Traditional form
x = 5
x = x + 1
y = x * 2

# SSA form (conceptual)
x_0 = 5
x_1 = x_0 + 1
y_0 = x_1 * 2
\end{lstlisting}

Why SSA matters:
\begin{itemize}
    \item \textbf{No ambiguity}: Each value has exactly one definition
    \item \textbf{Easy analysis}: Use-def chains are explicit
    \item \textbf{Better optimization}: Many optimizations become simpler
    \item \textbf{Parallel-friendly}: Dependencies are clear
\end{itemize}

\subsection{Your First xDSL Program}

Let's create a simple IR program using xDSL:

\begin{lstlisting}[language=Python, caption=Creating IR in xDSL]
from xdsl.context import Context
from xdsl.dialects import builtin, arith, func
from xdsl.ir import Operation, Block, Region
from xdsl.builder import Builder

# Create a context (like a namespace for dialects)
ctx = Context()
ctx.load_dialect(builtin.Builtin)
ctx.load_dialect(arith.Arith)
ctx.load_dialect(func.Func)

# Build a simple function that adds two numbers
@Builder.implicit_region
def create_add_function():
    # Create function signature: (i32, i32) -> i32
    func_type = func.FunctionType.from_lists([builtin.i32, builtin.i32], [builtin.i32])
    
    with func.FuncOp("add", func_type, visibility="public").body:
        # Get function arguments
        arg0 = Block.current().args[0]
        arg1 = Block.current().args[1]
        
        # Create add operation
        result = arith.Addi(arg0, arg1).result
        
        # Return the result
        func.Return(result)

module = builtin.ModuleOp([create_add_function()])
print(module)
\end{lstlisting}

\subsection{Understanding IR Structure}

The IR in xDSL follows a hierarchical structure:

\begin{tikzpicture}[node distance=1.5cm]
    \node[draw, rectangle] (module) {Module};
    \node[draw, rectangle, below of=module] (function) {Function};
    \node[draw, rectangle, below of=function] (block) {Block};
    \node[draw, rectangle, below of=block] (operation) {Operation};
    
    \draw[->] (module) -- (function) node[midway, right] {contains};
    \draw[->] (function) -- (block) node[midway, right] {has body};
    \draw[->] (block) -- (operation) node[midway, right] {contains};
\end{tikzpicture}

\subsection{Dialects: Modular IR Design}

xDSL organizes operations into \textit{dialects} - collections of related operations and types. This is similar to how PyTorch has different modules (nn, optim, etc.).

Common dialects in xDSL:
\begin{itemize}
    \item \texttt{arith}: Arithmetic operations (add, multiply, divide)
    \item \texttt{func}: Function definitions and calls
    \item \texttt{memref}: Memory operations (like NumPy arrays)
    \item \texttt{scf}: Structured control flow (loops, conditionals)
    \item \texttt{linalg}: Linear algebra operations (matrix multiply, convolution)
\end{itemize}

\section{Chapter 2: Operations and Operators}

\subsection{What is an Operation?}

An operation in xDSL is the fundamental unit of computation. Each operation:
\begin{itemize}
    \item Has a name (e.g., \texttt{arith.addi} for integer addition)
    \item Takes input values (operands)
    \item Produces output values (results)
    \item May have attributes (compile-time constants)
    \item Can contain regions (nested blocks of operations)
\end{itemize}

\subsection{Creating Custom Operations}

Let's define a custom operation for matrix operations:

\begin{lstlisting}[language=Python, caption=Defining Custom Operations]
from xdsl.irdl import (
    IRDLOperation, 
    OpResult, 
    Operand, 
    OperandDef, 
    ResultDef,
    op_def,
    result_def
)
from xdsl.dialects.builtin import TensorType

@op_def
class MatMulOp(IRDLOperation):
    """Matrix multiplication operation"""
    name = "my_dialect.matmul"
    
    # Define operands (inputs)
    lhs = operand_def(TensorType)
    rhs = operand_def(TensorType)
    
    # Define results (outputs)
    result = result_def(TensorType)
    
    def __init__(self, lhs: Value, rhs: Value):
        super().__init__(operands=[lhs, rhs], 
                        result_types=[infer_matmul_type(lhs, rhs)])
    
    def verify(self):
        """Verify that the operation is well-formed"""
        # Check that dimensions are compatible
        lhs_shape = self.lhs.type.shape
        rhs_shape = self.rhs.type.shape
        
        if lhs_shape[-1] != rhs_shape[0]:
            raise ValueError("Incompatible matrix dimensions")
\end{lstlisting}

\subsection{Operation Semantics}

Every operation has well-defined semantics - what it means to execute that operation:

\begin{lstlisting}[language=Python, caption=Operation Semantics Example]
# Arithmetic operations
add_op = arith.Addi(a, b)  # result = a + b
mul_op = arith.Muli(a, b)  # result = a * b

# Memory operations  
alloc_op = memref.Alloc.get(shape=[10, 20], element_type=f32)  # Allocate 10x20 array
load_op = memref.Load.get(memref_value, indices)  # Load from memory
store_op = memref.Store.get(value, memref_value, indices)  # Store to memory

# Control flow operations
for_op = scf.For(lower_bound, upper_bound, step) # for loop
if_op = scf.If(condition, has_else=True)  # if-then-else
\end{lstlisting}

\subsection{Attributes vs Operands}

Understanding the difference between attributes and operands is crucial:

\begin{itemize}
    \item \textbf{Attributes}: Compile-time constants (shapes, types, flags)
    \item \textbf{Operands}: Runtime values (variables, intermediate results)
\end{itemize}

\begin{lstlisting}[language=Python, caption=Attributes vs Operands]
# Constant has an attribute (the value)
const = arith.Constant.from_int_and_width(42, 32)  # 42 is an attribute

# Add has operands (runtime values)
result = arith.Addi(x, y)  # x and y are operands

# Alloc has attributes (shape) but no operands
mem = memref.Alloc.get(shape=[100], element_type=f32)  # shape is attribute
\end{lstlisting}

\section{Chapter 3: Analysis Passes}

\subsection{What is Analysis?}

Analysis passes extract information from IR without modifying it. They answer questions like:
\begin{itemize}
    \item Which variables are used where? (Use-Def Analysis)
    \item Which operations can run in parallel? (Dependency Analysis)
    \item What are the loop bounds? (Range Analysis)
    \item Which operations are dead code? (Liveness Analysis)
\end{itemize}

\subsection{Writing a Simple Analysis Pass}

Let's write an analysis that counts operations by type:

\begin{lstlisting}[language=Python, caption=Operation Counter Analysis]
from xdsl.passes import Pass
from xdsl.ir import Operation, OpResult
from collections import defaultdict

class OperationCounterPass(Pass):
    """Count operations by type in the IR"""
    
    name = "count-ops"
    
    def apply(self, module: builtin.ModuleOp) -> None:
        op_counts = defaultdict(int)
        
        # Walk through all operations in the module
        for op in module.walk():
            op_counts[op.name] += 1
        
        # Print analysis results
        print("Operation counts:")
        for op_name, count in sorted(op_counts.items()):
            print(f"  {op_name}: {count}")
        
        # Store results for use by other passes
        self.op_counts = op_counts
\end{lstlisting}

\subsection{Use-Def Chains}

Use-Def chains track where values are defined and used:

\begin{lstlisting}[language=Python, caption=Use-Def Analysis]
class UseDefAnalysis(Pass):
    """Analyze use-def relationships"""
    
    def apply(self, module: builtin.ModuleOp) -> None:
        # Build use-def chains
        for op in module.walk():
            for result in op.results:
                print(f"Value {result} defined by {op.name}")
                print(f"  Used by: {[use.operation.name for use in result.uses]}")
    
    def find_unused_values(self, module: builtin.ModuleOp) -> list[OpResult]:
        """Find values that are never used"""
        unused = []
        for op in module.walk():
            for result in op.results:
                if not result.uses:
                    unused.append(result)
        return unused
\end{lstlisting}

\subsection{Dominance Analysis}

Dominance tells us which operations must execute before others:

\begin{lstlisting}[language=Python, caption=Dominance Analysis]
from xdsl.ir import Block
from xdsl.irdl.dominance import DominanceInfo

class DominanceAnalysis(Pass):
    """Analyze control flow dominance"""
    
    def apply(self, module: builtin.ModuleOp) -> None:
        # For each function in the module
        for func_op in module.body.ops:
            if isinstance(func_op, func.FuncOp):
                dom_info = DominanceInfo(func_op.body)
                
                # Check dominance relationships
                for block in func_op.body.blocks:
                    dominators = dom_info.get_dominators(block)
                    print(f"Block {block} dominated by: {dominators}")
\end{lstlisting}

\subsection{Dataflow Analysis}

Dataflow analysis propagates information through the program:

\begin{lstlisting}[language=Python, caption=Constant Propagation Analysis]
class ConstantPropagationAnalysis(Pass):
    """Analyze which values are compile-time constants"""
    
    def apply(self, module: builtin.ModuleOp) -> None:
        known_constants = {}
        
        # First pass: identify constants
        for op in module.walk():
            if isinstance(op, arith.Constant):
                known_constants[op.result] = op.value.value
        
        # Iteratively propagate constants
        changed = True
        while changed:
            changed = False
            for op in module.walk():
                if isinstance(op, arith.Addi):
                    # If both operands are known constants
                    if (op.lhs in known_constants and 
                        op.rhs in known_constants):
                        result_value = (known_constants[op.lhs] + 
                                      known_constants[op.rhs])
                        if op.result not in known_constants:
                            known_constants[op.result] = result_value
                            changed = True
        
        return known_constants
\end{lstlisting}

\section{Chapter 4: Transformations and Optimizations}

\subsection{What is a Transformation?}

Transformations modify the IR to improve performance, reduce code size, or prepare for further optimizations. Unlike analysis passes, transformations actually change the program structure.

Key principles:
\begin{itemize}
    \item \textbf{Correctness}: Must preserve program semantics
    \item \textbf{Profitability}: Should improve some metric (speed, size, power)
    \item \textbf{Composability}: Should work well with other transformations
\end{itemize}

\subsection{Pattern-Based Rewriting}

xDSL provides powerful pattern matching for transformations:

\begin{lstlisting}[language=Python, caption=Simple Algebraic Optimization]
from xdsl.pattern_rewriter import (
    PatternRewriter, 
    RewritePattern,
    op_type_rewrite_pattern
)

class FoldAddZero(RewritePattern):
    """Fold x + 0 -> x"""
    
    @op_type_rewrite_pattern
    def match_and_rewrite(self, op: arith.Addi, 
                         rewriter: PatternRewriter) -> None:
        # Check if right operand is zero
        if isinstance(op.rhs.owner, arith.Constant):
            if op.rhs.owner.value.value == 0:
                # Replace all uses of the add with the left operand
                rewriter.replace_op(op, [op.lhs])
                return
        
        # Check if left operand is zero  
        if isinstance(op.lhs.owner, arith.Constant):
            if op.lhs.owner.value.value == 0:
                rewriter.replace_op(op, [op.rhs])
\end{lstlisting}

\subsection{Common Optimizations}

\subsubsection{Dead Code Elimination}

Remove operations whose results are never used:

\begin{lstlisting}[language=Python, caption=Dead Code Elimination]
class DeadCodeElimination(Pass):
    """Remove unused operations"""
    
    def apply(self, module: builtin.ModuleOp) -> None:
        ops_to_remove = []
        
        for op in module.walk():
            # Skip operations with side effects
            if self.has_side_effects(op):
                continue
                
            # Check if any results are used
            all_dead = all(not result.uses for result in op.results)
            
            if all_dead:
                ops_to_remove.append(op)
        
        # Remove dead operations
        for op in ops_to_remove:
            op.erase()
    
    def has_side_effects(self, op: Operation) -> bool:
        """Check if operation has side effects"""
        # Memory writes, function calls, etc.
        return isinstance(op, (memref.Store, func.Call))
\end{lstlisting}

\subsubsection{Common Subexpression Elimination}

Identify and eliminate redundant computations:

\begin{lstlisting}[language=Python, caption=Common Subexpression Elimination]
class CommonSubexpressionElimination(Pass):
    """Eliminate redundant computations"""
    
    def apply(self, module: builtin.ModuleOp) -> None:
        # Map from (op_type, operands) to result
        expression_map = {}
        
        for op in module.walk():
            if self.is_pure(op):
                # Create a key for this expression
                key = self.get_expression_key(op)
                
                if key in expression_map:
                    # Found duplicate - replace with existing
                    existing_result = expression_map[key]
                    op.results[0].replace_by(existing_result)
                    op.erase()
                else:
                    # First occurrence - remember it
                    expression_map[key] = op.results[0]
    
    def get_expression_key(self, op: Operation):
        """Create hashable key for expression"""
        return (op.name, tuple(op.operands))
\end{lstlisting}

\subsubsection{Loop Optimizations}

Optimize loop structures for better performance:

\begin{lstlisting}[language=Python, caption=Loop Unrolling]
class LoopUnrolling(Pass):
    """Unroll small loops"""
    
    def apply(self, module: builtin.ModuleOp) -> None:
        for op in module.walk():
            if isinstance(op, scf.For):
                if self.should_unroll(op):
                    self.unroll_loop(op)
    
    def should_unroll(self, loop: scf.For) -> bool:
        """Decide if loop should be unrolled"""
        # Get loop bounds if constant
        if (isinstance(loop.lb.owner, arith.Constant) and
            isinstance(loop.ub.owner, arith.Constant)):
            lb = loop.lb.owner.value.value
            ub = loop.ub.owner.value.value
            iterations = ub - lb
            
            # Unroll small loops
            return iterations <= 4
        return False
    
    def unroll_loop(self, loop: scf.For):
        """Completely unroll a loop"""
        # Clone loop body for each iteration
        # Update induction variable references
        # Remove original loop
        pass  # Implementation details omitted
\end{lstlisting}

\subsection{Lowering Transformations}

Lowering transforms high-level operations into simpler ones:

\begin{lstlisting}[language=Python, caption=Lowering High-Level Operations]
class LowerMatrixOps(Pass):
    """Lower matrix operations to loops"""
    
    def apply(self, module: builtin.ModuleOp) -> None:
        for op in module.walk():
            if isinstance(op, MatMulOp):
                self.lower_matmul(op)
    
    def lower_matmul(self, matmul: MatMulOp):
        """Lower matmul to three nested loops"""
        builder = Builder.before(matmul)
        
        # Get dimensions
        m, k = matmul.lhs.type.shape
        _, n = matmul.rhs.type.shape
        
        # Allocate result
        result = builder.insert(memref.Alloc.get([m, n], f32))
        
        # Generate three nested loops
        with builder.insert(scf.For(0, m, 1)) as i:
            with builder.insert(scf.For(0, n, 1)) as j:
                # Initialize accumulator
                zero = builder.insert(arith.Constant.from_float(0.0, f32))
                
                with builder.insert(scf.For(0, k, 1, [zero])) as (k_idx, acc):
                    # Load elements
                    a_elem = builder.insert(memref.Load(matmul.lhs, [i, k_idx]))
                    b_elem = builder.insert(memref.Load(matmul.rhs, [k_idx, j]))
                    
                    # Multiply and accumulate
                    prod = builder.insert(arith.Mulf(a_elem, b_elem))
                    new_acc = builder.insert(arith.Addf(acc, prod))
                    
                    scf.Yield(new_acc)
                
                # Store result
                builder.insert(memref.Store(acc, result, [i, j]))
        
        # Replace matmul with lowered result
        matmul.results[0].replace_by(result)
        matmul.erase()
\end{lstlisting}

\section{Chapter 5: Practical Examples}

\subsection{Building a Simple Expression Compiler}

Let's build a complete compiler for arithmetic expressions:

\begin{lstlisting}[language=Python, caption=Expression Compiler]
from dataclasses import dataclass
from typing import Union

# Define AST nodes
@dataclass
class Expr:
    pass

@dataclass
class Number(Expr):
    value: int

@dataclass  
class BinaryOp(Expr):
    left: Expr
    op: str
    right: Expr

# Parser (simplified)
def parse(text: str) -> Expr:
    """Parse expression like '2 + 3 * 4'"""
    # Implementation omitted for brevity
    pass

# IR Generator
class ExprIRGen:
    def __init__(self):
        self.builder = Builder()
        
    def compile(self, expr: Expr) -> OpResult:
        """Compile expression to xDSL IR"""
        if isinstance(expr, Number):
            return arith.Constant.from_int_and_width(
                expr.value, 32).result
        
        elif isinstance(expr, BinaryOp):
            left = self.compile(expr.left)
            right = self.compile(expr.right)
            
            if expr.op == '+':
                return arith.Addi(left, right).result
            elif expr.op == '*':
                return arith.Muli(left, right).result
            # ... other operations

# Optimizer
def optimize(module: builtin.ModuleOp):
    """Apply optimization passes"""
    passes = [
        ConstantFolding(),
        CommonSubexpressionElimination(),
        DeadCodeElimination(),
    ]
    
    for pass_instance in passes:
        pass_instance.apply(module)

# Complete pipeline
def compile_expression(text: str):
    # Parse
    ast = parse(text)
    
    # Generate IR
    ir_gen = ExprIRGen()
    result = ir_gen.compile(ast)
    
    # Wrap in module
    module = builtin.ModuleOp([
        func.FuncOp("compute", 
                   func.FunctionType.from_lists([], [builtin.i32]),
                   body=[func.Return(result)])
    ])
    
    # Optimize
    optimize(module)
    
    return module
\end{lstlisting}

\subsection{Implementing a Domain-Specific Optimization}

Let's create an optimization for neural network operations:

\begin{lstlisting}[language=Python, caption=Neural Network Optimization]
class FuseBatchNormIntoConv(Pass):
    """Fuse batch normalization into preceding convolution"""
    
    def apply(self, module: builtin.ModuleOp) -> None:
        for conv in module.walk():
            if isinstance(conv, nn.Conv2d):
                # Look for batch norm following conv
                for use in conv.result.uses:
                    if isinstance(use.operation, nn.BatchNorm2d):
                        self.fuse(conv, use.operation)
    
    def fuse(self, conv: nn.Conv2d, bn: nn.BatchNorm2d):
        """Fuse batch norm parameters into convolution"""
        # Extract batch norm parameters
        gamma = bn.gamma  # scale
        beta = bn.beta    # shift
        mean = bn.running_mean
        var = bn.running_var
        eps = bn.eps
        
        # Compute fused weights and bias
        std = sqrt(var + eps)
        fused_weight = conv.weight * (gamma / std)
        fused_bias = gamma * (conv.bias - mean) / std + beta
        
        # Create new convolution with fused parameters
        fused_conv = nn.Conv2d(
            weight=fused_weight,
            bias=fused_bias,
            # ... other conv parameters
        )
        
        # Replace conv -> bn with fused conv
        bn.result.replace_by(fused_conv.result)
        conv.erase()
        bn.erase()
\end{lstlisting}

\subsection{Building a Mini ML Compiler}

Let's create a compiler for a subset of ML operations:

\begin{lstlisting}[language=Python, caption=Mini ML Compiler]
# Define ML dialect
@dialect_def
class MLDialect(Dialect):
    name = "ml"

@op_def
class LinearOp(IRDLOperation):
    """Linear layer: y = Wx + b"""
    name = "ml.linear"
    
    input = operand_def(TensorType)
    weight = operand_def(TensorType)  
    bias = operand_def(TensorType)
    
    output = result_def(TensorType)

@op_def
class ReluOp(IRDLOperation):
    """ReLU activation"""
    name = "ml.relu"
    
    input = operand_def(TensorType)
    output = result_def(TensorType)

# Optimization: Fuse Linear + ReLU
class FuseLinearRelu(RewritePattern):
    @op_type_rewrite_pattern
    def match_and_rewrite(self, relu: ReluOp, 
                         rewriter: PatternRewriter):
        # Check if input is from linear
        if isinstance(relu.input.owner, LinearOp):
            linear = relu.input.owner
            
            # Create fused operation
            fused = LinearReluOp(
                linear.input, 
                linear.weight,
                linear.bias
            )
            
            # Replace both ops with fused version
            rewriter.replace_op(relu, [fused.output])
            rewriter.erase_op(linear)

# Lower to hardware operations
class LowerMLToHardware(Pass):
    """Lower ML ops to target hardware"""
    
    def apply(self, module: builtin.ModuleOp):
        target = self.get_target()  # CPU, GPU, TPU, etc.
        
        for op in module.walk():
            if isinstance(op, LinearOp):
                if target == "gpu":
                    self.lower_linear_to_cublas(op)
                elif target == "cpu":
                    self.lower_linear_to_loops(op)
\end{lstlisting}

\section{Chapter 6: Advanced Topics}

\subsection{Multi-Level IR and Progressive Lowering}

Real compilers use multiple levels of abstraction:

\begin{lstlisting}[language=Python, caption=Multi-Level Compilation Pipeline]
class CompilationPipeline:
    """Progressive lowering through multiple IR levels"""
    
    def compile(self, source_code: str) -> str:
        # Level 1: High-level ML operations
        ml_ir = parse_to_ml_dialect(source_code)
        ml_ir = optimize_ml_level(ml_ir)
        
        # Level 2: Linear algebra operations
        linalg_ir = lower_ml_to_linalg(ml_ir)
        linalg_ir = optimize_linalg_level(linalg_ir)
        
        # Level 3: Loops and memory operations
        loop_ir = lower_linalg_to_loops(linalg_ir)
        loop_ir = optimize_loops(loop_ir)
        
        # Level 4: Target-specific operations
        target_ir = lower_to_target(loop_ir)
        target_ir = optimize_target_specific(target_ir)
        
        # Code generation
        return generate_code(target_ir)
\end{lstlisting}

\subsection{Cost Models and Autotuning}

Choosing the best optimization requires understanding costs:

\begin{lstlisting}[language=Python, caption=Cost-Based Optimization]
class CostModel:
    """Estimate operation costs"""
    
    def estimate_cost(self, op: Operation) -> float:
        if isinstance(op, arith.Mulf):
            return 1.0  # FP multiply cost
        elif isinstance(op, memref.Load):
            return self.memory_latency(op)
        elif isinstance(op, scf.For):
            iterations = self.analyze_trip_count(op)
            body_cost = sum(self.estimate_cost(body_op) 
                          for body_op in op.body.walk())
            return iterations * body_cost

class AutotuningPass(Pass):
    """Try different optimization strategies"""
    
    def apply(self, module: builtin.ModuleOp):
        strategies = [
            self.try_vectorization,
            self.try_loop_tiling,
            self.try_parallelization,
        ]
        
        best_module = module
        best_cost = self.cost_model.estimate_cost(module)
        
        for strategy in strategies:
            candidate = strategy(module.clone())
            cost = self.cost_model.estimate_cost(candidate)
            
            if cost < best_cost:
                best_module = candidate
                best_cost = cost
        
        return best_module
\end{lstlisting}

\subsection{Connection to ML Compilation}

Modern ML frameworks are essentially compilers:

\begin{itemize}
    \item \textbf{PyTorch JIT}: Traces Python code to TorchScript IR
    \item \textbf{TensorFlow XLA}: Compiles TF graphs to optimized kernels
    \item \textbf{JAX}: Uses XLA for JIT compilation
    \item \textbf{Apache TVM}: Optimizes ML models for various hardware
\end{itemize}

The concepts you've learned apply directly:
\begin{itemize}
    \item \textbf{Graph optimization} = IR transformation
    \item \textbf{Kernel fusion} = Operation merging
    \item \textbf{Memory planning} = Register allocation
    \item \textbf{Quantization} = Type lowering
\end{itemize}

\section{Chapter 7: Hands-On Exercises}

\subsection{Introduction: Learning by Doing}

The exercises in this chapter are designed to solidify your understanding of compiler concepts through practical implementation. Each exercise builds upon the concepts we've covered, challenging you to apply your knowledge in increasingly sophisticated ways. These aren't just academic exercises - they mirror real-world compiler engineering challenges you'll encounter in production systems.

\textbf{Goals of These Exercises:}
\begin{itemize}
    \item \textbf{Reinforce Core Concepts}: Apply IR manipulation, pattern matching, and transformation techniques in practice
    \item \textbf{Develop Compiler Intuition}: Understand trade-offs between different optimization strategies
    \item \textbf{Build Debugging Skills}: Learn to verify correctness and debug transformation passes
    \item \textbf{Connect Theory to Practice}: See how abstract concepts translate to real optimizations
\end{itemize}

\subsection{Exercise 1: Build Your Own Optimizer - Store-Load Forwarding}

\subsubsection{Context and Motivation}

Memory operations are often bottlenecks in program performance. When a program stores a value to memory and immediately loads it back, we're wasting cycles on unnecessary memory traffic. Store-load forwarding is a crucial optimization in modern compilers and CPUs that eliminates these redundant operations.

\textbf{Real-World Impact:} This optimization is so important that modern CPUs implement it in hardware. By doing it in the compiler, we can eliminate the operations entirely, reducing both memory bandwidth and instruction count.

\textbf{What You'll Learn:}
\begin{itemize}
    \item How to track data flow through memory operations
    \item Alias analysis basics (when is it safe to forward?)
    \item The importance of maintaining program correctness during optimization
    \item How to reason about memory dependencies
\end{itemize}

\begin{lstlisting}[language=Python, caption=Exercise: Store-Load Forwarding]
class StoreLoadForwarding(Pass):
    """Forward stored values to subsequent loads
    
    Transform patterns like:
        memref.store %value, %mem[%i, %j]
        %loaded = memref.load %mem[%i, %j]
    Into:
        memref.store %value, %mem[%i, %j]
        // Use %value directly instead of %loaded
    """
    
    def apply(self, module: builtin.ModuleOp):
        # TODO: Your implementation here
        # Step 1: Build a map of recent stores for each memory location
        # Step 2: For each load, check if we have a recent store
        # Step 3: Verify indices match exactly (be conservative!)
        # Step 4: Check for intervening operations that might invalidate
        # Step 5: Replace load result with stored value
        
        # Hints:
        # - Start simple: handle only same-block forwarding
        # - Be conservative: only forward when indices are identical
        # - Watch for: function calls, other stores to same array
        # - Remember: correctness > performance
        pass

    def can_forward(self, store_op, load_op):
        """Check if it's safe to forward from store to load"""
        # TODO: Implement safety checks
        pass
\end{lstlisting}

\subsubsection{Testing Your Implementation}

\begin{lstlisting}[language=Python, caption=Test Cases for Store-Load Forwarding]
def test_store_load_forwarding():
    """Test your optimization works correctly"""
    
    # Test case 1: Simple forwarding
    # store 42 to mem[0]
    # load from mem[0]  <-- should be replaced with 42
    
    # Test case 2: Different indices (no forwarding)
    # store 42 to mem[0]
    # load from mem[1]  <-- should NOT be forwarded
    
    # Test case 3: Intervening store
    # store 42 to mem[0]
    # store 100 to mem[0]
    # load from mem[0]  <-- should get 100, not 42
    
    # Test case 4: Across basic blocks (advanced)
    # Requires dominance analysis
\end{lstlisting}

\subsection{Exercise 2: Create a Custom Dialect - Quantum Computing}

\subsubsection{Context and Motivation}

Quantum computing represents a fundamentally different computational model. By creating a quantum dialect, you'll learn how compiler infrastructure can be extended to support entirely new paradigms. This exercise demonstrates xDSL's extensibility and teaches you to think about computation abstractly.

\textbf{Why This Matters:} As new computational paradigms emerge (quantum, neuromorphic, photonic), the ability to quickly prototype new compiler infrastructure becomes crucial. This exercise teaches you the skills needed to support future computing platforms.

\textbf{What You'll Learn:}
\begin{itemize}
    \item How to design operations for non-standard computation models
    \item Type system design for domain-specific constraints
    \item Verification of domain-specific invariants
    \item Lowering from high-level domain concepts to implementation
\end{itemize}

\begin{lstlisting}[language=Python, caption=Exercise: Quantum Dialect]
from xdsl.irdl import irdl_op_definition, irdl_attr_definition
from xdsl.ir import OpResult, SSAValue

@dialect_def
class QuantumDialect(Dialect):
    name = "quantum"

# TODO: Define quantum-specific types
@irdl_attr_definition
class QubitType(TypeAttribute):
    """Type representing a quantum bit"""
    name = "quantum.qubit"

@irdl_attr_definition  
class QuantumRegisterType(TypeAttribute):
    """Type for quantum register of n qubits"""
    name = "quantum.qreg"
    size: int

# TODO: Define quantum gates
@irdl_op_definition
class HadamardOp(IRDLOperation):
    """Hadamard gate: creates superposition"""
    name = "quantum.h"
    
    input_qubit = operand_def(QubitType)
    output_qubit = result_def(QubitType)
    
    def verify(self):
        """Verify operation is well-formed"""
        # Hadamard is always valid on a single qubit
        pass

@irdl_op_definition
class CNOTOp(IRDLOperation):
    """Controlled-NOT gate"""
    name = "quantum.cnot"
    
    control = operand_def(QubitType)
    target = operand_def(QubitType)
    
    # CNOT modifies both qubits
    new_control = result_def(QubitType)
    new_target = result_def(QubitType)

@irdl_op_definition
class MeasureOp(IRDLOperation):
    """Measure qubit, collapsing to classical bit"""
    name = "quantum.measure"
    
    qubit = operand_def(QubitType)
    classical_bit = result_def(IntegerType(1))

# TODO: Optimization passes
class QuantumCircuitOptimizer(Pass):
    """Optimize quantum circuits"""
    
    def apply(self, module: builtin.ModuleOp):
        # TODO: Implement optimizations like:
        # - Gate cancellation (H·H = I)
        # - Gate commutation 
        # - Circuit depth reduction
        pass
\end{lstlisting}

\subsubsection{Challenge Extensions}
\begin{itemize}
    \item Implement quantum circuit simulation
    \item Add error correction operations
    \item Create a cost model for quantum operations
    \item Implement decomposition to basic gate sets
\end{itemize}

\subsection{Exercise 3: Implement Vectorization}

\subsubsection{Context and Motivation}

Modern processors have SIMD (Single Instruction, Multiple Data) units that can process multiple data elements simultaneously. Vectorization transforms scalar operations to leverage these capabilities, often providing 4-8x speedups. This is a cornerstone optimization in high-performance computing.

\textbf{Industry Relevance:} Every production compiler implements auto-vectorization. ML frameworks rely heavily on vectorized operations for performance. Understanding vectorization is essential for performance engineering.

\textbf{What You'll Learn:}
\begin{itemize}
    \item How to identify vectorizable patterns
    \item Dependency analysis for parallel execution
    \item Handling edge cases (loop remainders, alignment)
    \item Cost modeling for vectorization decisions
\end{itemize}

\begin{lstlisting}[language=Python, caption=Exercise: Auto-Vectorization]
class Vectorization(Pass):
    """Automatically vectorize loops
    
    Transform:
        for i in range(n):
            c[i] = a[i] + b[i]
    
    Into:
        for i in range(0, n-4, 4):
            c[i:i+4] = a[i:i+4] + b[i:i+4]  # Vector operation
        for i in range(n-n%4, n):
            c[i] = a[i] + b[i]  # Scalar remainder
    """
    
    def __init__(self, vector_width=4):
        self.vector_width = vector_width
    
    def apply(self, module: builtin.ModuleOp):
        for loop in self.find_loops(module):
            if self.is_vectorizable(loop):
                self.vectorize_loop(loop)
    
    def is_vectorizable(self, loop: scf.For) -> bool:
        """Check if loop can be vectorized"""
        # TODO: Check for:
        # 1. No loop-carried dependencies
        # 2. Contiguous memory access
        # 3. Supported operations (add, mul, etc.)
        # 4. No control flow in loop body
        pass
    
    def vectorize_loop(self, loop: scf.For):
        """Transform loop to use vector operations"""
        # TODO: Implementation steps:
        # 1. Create vector loop with stride = vector_width
        # 2. Replace scalar ops with vector ops
        # 3. Create remainder loop for n % vector_width iterations
        # 4. Update memory operations to vector loads/stores
        pass
    
    def analyze_dependencies(self, loop: scf.For) -> bool:
        """Check for loop-carried dependencies"""
        # TODO: Implement dependency analysis
        # Look for patterns like a[i] = a[i-1] + b[i]
        pass
\end{lstlisting}

\subsubsection{Advanced Challenges}
\begin{itemize}
    \item Handle non-unit stride access patterns
    \item Implement cost model to decide when vectorization is profitable
    \item Support reduction operations (sum, max)
    \item Handle conditional operations with masking
\end{itemize}

\subsection{Exercise 4: Build a Simple JIT Compiler}

\subsubsection{Context and Motivation}

Just-In-Time compilation bridges the gap between interpretation and ahead-of-time compilation. By compiling code at runtime, JIT compilers can optimize based on actual execution patterns. This exercise gives you hands-on experience with the full compilation pipeline.

\textbf{Real-World Applications:} JIT compilation powers JavaScript engines (V8), Java VMs (HotSpot), Python (PyPy), and ML frameworks (JAX, PyTorch). Understanding JIT compilation is crucial for modern system design.

\textbf{What You'll Learn:}
\begin{itemize}
    \item The complete compilation pipeline from source to execution
    \item Runtime code generation techniques
    \item Optimization decisions based on runtime information
    \item Integration between compiled and interpreted code
\end{itemize}

\begin{lstlisting}[language=Python, caption=Exercise: JIT Compiler]
import ctypes
from typing import Dict, Callable

class ExpressionJIT:
    """JIT compile and execute mathematical expressions
    
    This JIT compiler will:
    1. Parse expressions into AST
    2. Generate xDSL IR
    3. Optimize the IR
    4. Generate executable code
    5. Return a callable Python function
    """
    
    def __init__(self):
        self.cache: Dict[str, Callable] = {}
        self.optimization_level = 2
    
    def compile(self, expr_string: str) -> Callable:
        """Compile expression to native function"""
        
        # Check cache first
        if expr_string in self.cache:
            return self.cache[expr_string]
        
        # Step 1: Parse expression
        ast = self.parse_expression(expr_string)
        
        # Step 2: Generate IR
        ir_module = self.generate_ir(ast)
        
        # Step 3: Optimize
        if self.optimization_level > 0:
            ir_module = self.optimize_ir(ir_module)
        
        # Step 4: Generate machine code
        machine_code = self.generate_code(ir_module)
        
        # Step 5: Create callable wrapper
        func = self.create_callable(machine_code)
        
        # Cache for future use
        self.cache[expr_string] = func
        
        return func
    
    def parse_expression(self, expr: str):
        """Parse expression into AST"""
        # TODO: Implement expression parser
        # Support: numbers, variables, +, -, *, /, parentheses
        # Example: "x * 2 + y" -> BinaryOp('+', BinaryOp('*', Var('x'), Num(2)), Var('y'))
        pass
    
    def generate_ir(self, ast) -> builtin.ModuleOp:
        """Generate xDSL IR from AST"""
        # TODO: Create IR module with function
        # Function should take dict of variables, return result
        pass
    
    def optimize_ir(self, module: builtin.ModuleOp) -> builtin.ModuleOp:
        """Apply optimization passes"""
        passes = [
            ConstantFolding(),
            CommonSubexpressionElimination(),
            DeadCodeElimination(),
        ]
        
        for opt_pass in passes:
            opt_pass.apply(module)
        
        return module
    
    def generate_code(self, module: builtin.ModuleOp) -> bytes:
        """Generate machine code or Python bytecode"""
        # TODO: Options:
        # 1. Generate Python bytecode using 'compile()'
        # 2. Use LLVM bindings to generate native code
        # 3. Generate C code and compile with ctypes
        pass
    
    def create_callable(self, code: bytes) -> Callable:
        """Create Python-callable function from machine code"""
        # TODO: Wrap machine code in Python-callable interface
        pass

# Usage example:
jit = ExpressionJIT()
f = jit.compile("x * 2 + y")
result = f(x=5, y=3)  # Should return 13

# Advanced: Support for conditionals
g = jit.compile("x > 0 ? x * 2 : -x")
result2 = g(x=-5)  # Should return 5
\end{lstlisting}

\subsubsection{Implementation Hints}

\begin{lstlisting}[language=Python, caption=JIT Implementation Starter Code]
# Simple approach using Python's compile()
def generate_python_code(self, module: builtin.ModuleOp) -> str:
    """Generate Python source from IR"""
    code = "def compiled_func(**variables):\n"
    # Walk IR and generate Python code
    # ...
    return code

def create_callable_simple(self, py_code: str) -> Callable:
    """Create function from Python code"""
    exec_globals = {}
    exec(py_code, exec_globals)
    return exec_globals['compiled_func']
\end{lstlisting}

\section{Chapter 8: Real-World Applications}

\subsection{Case Study 1: Optimizing Neural Network Inference}

Let's optimize a real neural network for deployment:

\begin{lstlisting}[language=Python, caption=Neural Network Optimization Pipeline]
def optimize_nn_for_deployment(model: NeuralNetwork):
    """Optimize neural network for inference"""
    
    # Convert to IR
    ir = convert_nn_to_ir(model)
    
    # Apply optimizations
    optimizations = [
        # Structural optimizations
        FuseLayers(),           # Conv+BN, Linear+ReLU
        RemoveDropout(),        # Not needed for inference
        
        # Quantization
        QuantizeWeights(bits=8),
        QuantizeActivations(bits=8),
        
        # Memory optimizations
        InplaceOperations(),    # Reuse buffers
        MemoryPlanning(),       # Minimize allocation
        
        # Target-specific
        UseTargetIntrinsics(),  # AVX, NEON, etc.
        AutoTuneKernels(),      # Platform-specific tuning
    ]
    
    for opt in optimizations:
        ir = opt.apply(ir)
    
    return ir
\end{lstlisting}

\subsection{Case Study 2: Domain-Specific Language Compiler}

Build a compiler for financial computations:

\begin{lstlisting}[language=Python, caption=Financial DSL Compiler]
# Define financial operations dialect
@dialect_def
class FinanceDialect(Dialect):
    name = "finance"
    
    @op_def
    class BlackScholesOp(IRDLOperation):
        """Black-Scholes option pricing"""
        name = "finance.black_scholes"
        
        spot = operand_def(f64)
        strike = operand_def(f64)
        time = operand_def(f64)
        rate = operand_def(f64)
        volatility = operand_def(f64)
        
        call_price = result_def(f64)
        put_price = result_def(f64)

# Optimize for numerical stability
class NumericalStabilityPass(Pass):
    """Ensure numerical stability in financial calculations"""
    
    def apply(self, module: builtin.ModuleOp):
        for op in module.walk():
            if isinstance(op, math.ExpOp):
                # Prevent overflow/underflow
                self.add_range_check(op)
            elif isinstance(op, arith.DivfOp):
                # Prevent division by zero
                self.add_zero_check(op)
\end{lstlisting}

\subsection{Case Study 3: Compiler for Distributed Computing}

Compile programs for distributed execution:

\begin{lstlisting}[language=Python, caption=Distributed Computing Compiler]
@dialect_def
class DistributedDialect(Dialect):
    name = "dist"
    
    @op_def
    class AllReduceOp(IRDLOperation):
        """All-reduce across distributed nodes"""
        name = "dist.allreduce"
        
        input = operand_def(TensorType)
        output = result_def(TensorType)
        reduction = attr_def(str)  # "sum", "max", etc.

class DistributionPass(Pass):
    """Distribute computation across nodes"""
    
    def apply(self, module: builtin.ModuleOp):
        # Analyze data dependencies
        deps = self.analyze_dependencies(module)
        
        # Partition computation
        partitions = self.partition_computation(module, deps)
        
        # Insert communication operations
        for partition in partitions:
            self.insert_communication_ops(partition)
        
        # Generate distributed code
        return self.generate_distributed_code(partitions)
\end{lstlisting}

\section{Project: Building a Matrix Operations Compiler}

\subsection{Project Overview}

In this capstone project, you'll build a complete compiler for a domain-specific language focused on matrix operations. The goal is to take a Python-like program that performs matrix operations (including transpose and the inefficient double transpose pattern), parse it, create a custom MLIR dialect, generate IR, and implement an optimization pass that eliminates redundant double transposes.

A complete implementation of this compiler is available at: \url{https://github.com/djtodoro/matrix-toy-lang}

\textbf{Learning Objectives:}
\begin{itemize}
    \item Integrate all concepts from the course into a complete compiler
    \item Experience the full compiler development workflow from parsing to optimization
    \item Build a custom MLIR dialect using xDSL framework
    \item Implement pattern-based optimizations (double transpose elimination)
    \item Create a command-line compiler tool with real input/output
    \item Practice modular compiler design with separate parsing, IR generation, and optimization phases
\end{itemize}

\textbf{Key Components:}
\begin{enumerate}
    \item \textbf{Parser}: Python AST-based parser that extracts matrix operations from a custom `.mx` file format
    \item \textbf{Matrix Dialect}: Custom MLIR dialect with operations for matrix allocation, addition, multiplication, and transpose
    \item \textbf{IR Generator}: Converts parsed operations to MLIR IR using the matrix dialect
    \item \textbf{Optimizer}: Pattern rewriter that eliminates double transpose operations
    \item \textbf{Compiler Driver}: Command-line tool (\texttt{matrixc}) that orchestrates the compilation pipeline
\end{enumerate}

\subsection{Part 1: The Input Program}

The compiler accepts input files with a `.mx` extension containing Python-like matrix operations. Here's an example input program that demonstrates the double transpose optimization:

\begin{lstlisting}[language=Python, caption=main\_input.mx - Example Input Program]
"""
Simple double transpose optimization demonstration.
"""

def matrix_computation(A, B):
    """
    Demonstrate double transpose optimization patterns.
    
    Args:
        A: First input matrix
        B: Second input matrix
    
    Returns:
        Result matrix after operations
    """
    # Double transpose on A (should be optimized to just A)
    C = A.T.T
    
    # Another double transpose pattern on B
    D = B.T
    E = D.T  # E should equal B
    
    # Add the results
    F = C + E
    
    # Final double transpose
    result = F.T.T
    
    return result

def main():
    """Main function to test matrix computation."""
    # Define two 3x3 matrices
    A = matrix([[1, 2, 3],
                [4, 5, 6],
                [7, 8, 9]])
    
    B = matrix([[9, 8, 7],
                [6, 5, 4],
                [3, 2, 1]])

    # Run the computation
    result = matrix_computation(A, B)

\end{lstlisting}

\subsection{Compilation Example}

When the above program is compiled, the compiler performs several transformations:

\textbf{Initial IR (without optimization):}
\begin{lstlisting}[caption=Unoptimized IR with Double Transposes]
func.func @matrix_computation(%A : !matrix.type<4, 4, f32>, 
                               %B : !matrix.type<4, 4, f32>) 
                               -> !matrix.type<4, 4, f32> {
    %0 = "matrix.transpose"(%A) : (!matrix.type<4, 4, f32>) 
                                  -> !matrix.type<4, 4, f32>
    %1 = "matrix.transpose"(%0) : (!matrix.type<4, 4, f32>) 
                                  -> !matrix.type<4, 4, f32>  // A.T.T
    %2 = "matrix.transpose"(%B) : (!matrix.type<4, 4, f32>) 
                                  -> !matrix.type<4, 4, f32>
    %3 = "matrix.transpose"(%2) : (!matrix.type<4, 4, f32>) 
                                  -> !matrix.type<4, 4, f32>  // B.T.T
    %4 = "matrix.add"(%1, %3) : (!matrix.type<4, 4, f32>, 
                                 !matrix.type<4, 4, f32>) 
                                -> !matrix.type<4, 4, f32>
    %5 = "matrix.transpose"(%4) : (!matrix.type<4, 4, f32>) 
                                  -> !matrix.type<4, 4, f32>
    %6 = "matrix.transpose"(%5) : (!matrix.type<4, 4, f32>) 
                                  -> !matrix.type<4, 4, f32>  // result.T.T
    func.return %6 : !matrix.type<4, 4, f32>
}
\end{lstlisting}

\textbf{Optimized IR (after double transpose elimination):}
\begin{lstlisting}[caption=Optimized IR with Transposes Eliminated]
func.func @matrix_computation(%A : !matrix.type<4, 4, f32>, 
                               %B : !matrix.type<4, 4, f32>) 
                               -> !matrix.type<4, 4, f32> {
    %0 = "matrix.add"(%A, %B) : (!matrix.type<4, 4, f32>, 
                                 !matrix.type<4, 4, f32>) 
                                -> !matrix.type<4, 4, f32>
    func.return %0 : !matrix.type<4, 4, f32>
}
\end{lstlisting}

The optimization pass successfully eliminated all three double transpose patterns, reducing the operation count from 7 to 1.

\subsection{Part 2: Parser Implementation}

The parser uses Python's built-in AST module to extract matrix operations from the input `.mx` files. The implementation creates a visitor that walks the AST and identifies matrix-specific operations.

\textbf{Key Parser Features:}
\begin{itemize}
    \item Function-based operation extraction (tracks operations per function)
    \item Pattern recognition for double transpose operations
    \item Variable tracking across assignments
    \item Support for matrix literals using `matrix([...])` syntax
\end{itemize}

The parser implementation (`src/parser.py`) provides:

\begin{lstlisting}[language=Python, caption=Parser Interface]
# Main parser entry point
def parse_matrix_program(source_code: str) -> Dict[str, Any]:
    """Parse .mx source code and extract matrix operations by function."""
    tree = ast.parse(source_code)
    extractor = MatrixOperationExtractor()
    extractor.visit(tree)
    return extractor.functions  # Returns operations grouped by function
\end{lstlisting}

\subsection{Part 3: Matrix Operations Dialect}

The matrix dialect defines custom MLIR operations for matrix computations. The implementation uses xDSL's IRDL (IR Definition Language) to define operations with proper type checking and verification.

\textbf{Dialect Operations:}
\begin{itemize}
    \item \texttt{matrix.matmul}: Matrix multiplication with dimension compatibility verification
    \item \texttt{matrix.transpose}: Matrix transpose that swaps row/column dimensions
    \item \texttt{matrix.add}: Element-wise addition with shape verification
    \item \texttt{matrix.scalar\_mul}: Scalar multiplication of all matrix elements
    \item \texttt{matrix.alloc}: Matrix allocation with specified dimensions
\end{itemize}

\begin{lstlisting}[language=Python, caption=Dialect Structure Overview]
from xdsl.irdl import (
    IRDLOperation,
    irdl_op_definition,
    irdl_attr_definition,
    ParametrizedAttribute
)
from xdsl.dialects.builtin import Float32Type, IntAttr, TypeAttribute

# Custom matrix type with dimensions and element type
@irdl_attr_definition
class MatrixType(ParametrizedAttribute, TypeAttribute):
    """Type for matrix with known dimensions."""
    name = "matrix.type"
    # Parameters: rows, cols, dtype (e.g., !matrix.type<4, 4, f32>)

@irdl_op_definition
class MatMulOp(IRDLOperation):
    """Matrix multiplication operation."""
    name = "matrix.matmul"
    
    lhs = operand_def(MatrixType)
    rhs = operand_def(MatrixType)
    result = result_def(MatrixType)
    
    def __init__(self, lhs: SSAValue, rhs: SSAValue):
        # Infer result type from operand shapes
        lhs_type = lhs.type
        rhs_type = rhs.type
        result_type = MatrixType(
            lhs_type.rows.value,
            rhs_type.cols.value,
            lhs_type.dtype
        )
        super().__init__(
            operands=[lhs, rhs],
            result_types=[result_type]
        )
    
    def verify(self):
        """Verify that matrix dimensions are compatible."""
        if self.lhs.type.cols.value != self.rhs.type.rows.value:
            raise ValueError(
                f"Incompatible matrix dimensions for multiplication: "
                f"({self.lhs.type.rows.value}x{self.lhs.type.cols.value}) @ "
                f"({self.rhs.type.rows.value}x{self.rhs.type.cols.value})"
            )

@irdl_op_definition
class TransposeOp(IRDLOperation):
    """Matrix transpose operation."""
    name = "matrix.transpose"
    
    input = operand_def(MatrixType)
    result = result_def(MatrixType)
    
    def __init__(self, input: SSAValue):
        input_type = input.type
        # Transpose swaps rows and columns
        result_type = MatrixType(
            input_type.cols.value,
            input_type.rows.value,
            input_type.dtype
        )
        super().__init__(
            operands=[input],
            result_types=[result_type]
        )

@irdl_op_definition
class ScalarMulOp(IRDLOperation):
    """Element-wise scalar multiplication."""
    name = "matrix.scalar_mul"
    
    matrix = operand_def(MatrixType)
    scalar = attr_def(Float32Type)
    result = result_def(MatrixType)
    
    def __init__(self, matrix: SSAValue, scalar: float):
        super().__init__(
            operands=[matrix],
            attributes={'scalar': FloatAttr(scalar, Float32Type())},
            result_types=[matrix.type]  # Same shape as input
        )

@irdl_op_definition
class AddOp(IRDLOperation):
    """Element-wise matrix addition."""
    name = "matrix.add"
    
    lhs = operand_def(MatrixType)
    rhs = operand_def(MatrixType)
    result = result_def(MatrixType)
    
    def __init__(self, lhs: SSAValue, rhs: SSAValue):
        super().__init__(
            operands=[lhs, rhs],
            result_types=[lhs.type]  # Same shape as inputs
        )
    
    def verify(self):
        """Verify that matrices have the same shape."""
        if (self.lhs.type.rows.value != self.rhs.type.rows.value or
            self.lhs.type.cols.value != self.rhs.type.cols.value):
            raise ValueError("Matrix addition requires same dimensions")

@irdl_op_definition
class AllocOp(IRDLOperation):
    """Allocate matrix in memory."""
    name = "matrix.alloc"
    
    shape = attr_def(ArrayAttr)
    result = result_def(MatrixType)
    
    def __init__(self, rows: int, cols: int, dtype: TypeAttribute):
        matrix_type = MatrixType(rows, cols, dtype)
        super().__init__(
            operands=[],
            attributes={'shape': ArrayAttr([IntAttr(rows), IntAttr(cols)])},
            result_types=[matrix_type]
        )

class MatrixDialect(Dialect):
    """Dialect for matrix operations."""
    name = "matrix"
    
    operations = [
        MatMulOp,
        TransposeOp,
        ScalarMulOp,
        AddOp,
        AllocOp
    ]
\end{lstlisting}

\subsection{Part 4: IR Generation}

Convert the parsed Python AST to your matrix dialect IR:

\begin{lstlisting}[language=Python, caption=src/ir\_generator.py - IR Generation]
from xdsl.builder import Builder
from xdsl.dialects import builtin, func
from xdsl.ir import Block, Region
from typing import Dict, List, Any

class MatrixIRGenerator:
    """Generate matrix dialect IR from parsed operations."""
    
    def __init__(self):
        self.builder = Builder()
        self.var_map: Dict[str, SSAValue] = {}
    
    def generate(self, operations: List[Dict[str, Any]]) -> builtin.ModuleOp:
        """Generate IR module from operations list."""
        
        # Create function type (simplified: 2 matrices in, 1 out)
        func_type = func.FunctionType.from_lists(
            [MatrixType(None, None, Float32Type()),  # A
             MatrixType(None, None, Float32Type())], # B
            [MatrixType(None, None, Float32Type())]  # Result
        )
        
        # Create function
        with func.FuncOp("matrix_computation", func_type).body as body:
            # Map parameters to variables
            self.var_map['A'] = body.block.args[0]
            self.var_map['B'] = body.block.args[1]
            
            # Generate operations
            for op_info in operations:
                self.generate_operation(op_info)
            
            # Return final result
            result = self.var_map.get('result', self.var_map['A'])
            func.Return(result)
        
        # Create module
        return builtin.ModuleOp([body.owner])
    
    def generate_operation(self, op_info: Dict[str, Any]):
        """Generate IR for a single operation."""
        op_type = op_info['operation']['op']
        target = op_info['target']
        
        if op_type == 'matmul':
            left = self.var_map[op_info['operation']['left']]
            right = self.var_map[op_info['operation']['right']]
            result = MatMulOp(left, right).result
            self.var_map[target] = result
        
        elif op_type == 'transpose':
            matrix = self.var_map[op_info['operation']['matrix']]
            result = TransposeOp(matrix).result
            self.var_map[target] = result
        
        elif op_type == 'double_transpose':
            # This is where optimization opportunity exists!
            # For now, generate two transposes (will be optimized later)
            matrix = self.var_map[op_info['operation']['matrix']]
            temp = TransposeOp(matrix).result
            result = TransposeOp(temp).result
            self.var_map[target] = result
        
        elif op_type == 'scalar_mul':
            matrix = self.var_map[op_info['operation']['matrix']]
            scalar = op_info['operation']['scalar']
            result = ScalarMulOp(matrix, scalar).result
            self.var_map[target] = result
        
        elif op_type == 'add':
            left = self.var_map[op_info['operation']['left']]
            right = self.var_map[op_info['operation']['right']]
            result = AddOp(left, right).result
            self.var_map[target] = result
\end{lstlisting}

\subsection{Part 5: The Optimization Pass}

Implement the double transpose elimination optimization:

\begin{lstlisting}[language=Python, caption=src/middle-end.py - Double Transpose Elimination]
from xdsl.passes import Pass
from xdsl.pattern_rewriter import (
    PatternRewriter,
    RewritePattern,
    op_type_rewrite_pattern
)
from xdsl.ir import Operation
from typing import List

class DoubleTransposeElimination(RewritePattern):
    """Eliminate double transpose patterns: (A^T)^T = A"""
    
    @op_type_rewrite_pattern
    def match_and_rewrite(self, op: TransposeOp, 
                         rewriter: PatternRewriter) -> None:
        """Match and eliminate double transpose."""
        
        # Check if input to this transpose is also a transpose
        input_op = op.input.owner
        
        if isinstance(input_op, TransposeOp):
            # Found pattern: transpose(transpose(X)) = X
            original_matrix = input_op.input
            
            # Replace all uses of double transpose with original
            rewriter.replace_op(op, [original_matrix])
            
            # If the intermediate transpose has no other uses, remove it
            if not input_op.result.uses:
                rewriter.erase_op(input_op)
            
            print(f"Eliminated double transpose: {op}")

class TransposeChainOptimization(Pass):
    """Optimize chains of transpose operations."""
    
    name = "optimize-transpose"
    
    def apply(self, module: builtin.ModuleOp) -> None:
        """Apply transpose optimizations to module."""
        
        # Track statistics
        self.transposes_eliminated = 0
        
        # First pass: eliminate obvious double transposes
        pattern = DoubleTransposeElimination()
        PatternRewriter(module).apply_pattern(pattern)
        
        # Second pass: find hidden transpose chains
        self.find_transpose_chains(module)
        
        # Report results
        if self.transposes_eliminated > 0:
            print(f"Eliminated {self.transposes_eliminated} transpose operations")
    
    def find_transpose_chains(self, module: builtin.ModuleOp):
        """Find and optimize transpose chains across assignments."""
        
        # Build use-def chains
        transpose_ops = []
        for op in module.walk():
            if isinstance(op, TransposeOp):
                transpose_ops.append(op)
        
        # Analyze patterns
        for t_op in transpose_ops:
            if self.can_eliminate_transpose(t_op):
                self.eliminate_transpose(t_op)
                self.transposes_eliminated += 1
    
    def can_eliminate_transpose(self, op: TransposeOp) -> bool:
        """Check if transpose can be eliminated."""
        # Look for patterns where transpose cancels out
        # This is simplified - you should implement full analysis
        return False  # TODO: Implement
    
    def eliminate_transpose(self, op: TransposeOp):
        """Eliminate unnecessary transpose."""
        # TODO: Implement elimination logic
        pass

class MatrixOptimizationPipeline(Pass):
    """Complete optimization pipeline for matrix operations."""
    
    name = "matrix-opt-pipeline"
    
    def apply(self, module: builtin.ModuleOp) -> None:
        """Apply all matrix optimizations."""
        
        passes = [
            TransposeChainOptimization(),
            # Add more optimization passes here:
            # CommonSubexpressionElimination(),
            # DeadCodeElimination(),
            # MatrixChainOptimization(),  # Optimal parenthesization
        ]
        
        for opt_pass in passes:
            opt_pass.apply(module)
        
        # Verify IR is still valid
        module.verify()
\end{lstlisting}

\subsection{Part 6: Putting It All Together}

Here's the main compiler driver that combines all components:

\begin{lstlisting}[language=Python, caption=compiler.py - Main Compiler Driver]
#!/usr/bin/env python3
"""
Matrix Operations Compiler
A complete compiler for matrix operations with optimization.
"""

import sys
import argparse
from pathlib import Path

from parser import parse_matrix_program
from matrix_dialect import MatrixDialect
from ir_generator import MatrixIRGenerator
from optimize_transpose import MatrixOptimizationPipeline

from xdsl.context import Context
from xdsl.printer import Printer
from xdsl.dialects import builtin, func

class MatrixCompiler:
    """Main compiler class."""
    
    def __init__(self):
        # Initialize xDSL context
        self.ctx = Context()
        self.ctx.load_dialect(builtin.Builtin)
        self.ctx.load_dialect(func.Func)
        self.ctx.load_dialect(MatrixDialect)
        
        self.printer = Printer()
    
    def compile_file(self, input_file: Path, 
                    output_file: Path = None,
                    optimize: bool = True) -> builtin.ModuleOp:
        """Compile a Python file with matrix operations."""
        
        print(f"Compiling {input_file}...")
        
        # Step 1: Parse Python source
        with open(input_file, 'r') as f:
            source_code = f.read()
        
        operations = parse_matrix_program(source_code)
        print(f"Parsed {len(operations)} operations")
        
        # Step 2: Generate IR
        generator = MatrixIRGenerator()
        module = generator.generate(operations)
        print("Generated initial IR")
        
        if not optimize:
            print("Skipping optimizations")
        else:
            # Step 3: Optimize
            print("Running optimization passes...")
            optimizer = MatrixOptimizationPipeline()
            optimizer.apply(module)
        
        # Step 4: Output IR
        if output_file:
            with open(output_file, 'w') as f:
                self.printer.print_op(module, f)
            print(f"Wrote optimized IR to {output_file}")
        else:
            print("\nGenerated IR:")
            self.printer.print_op(module)
        
        return module
    
    def verify_optimization(self, original: str, optimized: str):
        """Verify that optimization preserves semantics."""
        # TODO: Implement semantic equivalence checking
        # Could involve:
        # - Symbolic execution
        # - Testing with concrete values
        # - Formal verification
        pass

def main():
    parser = argparse.ArgumentParser(
        description="Compile Python matrix operations to optimized IR"
    )
    parser.add_argument(
        'input',
        help='Input Python file with matrix operations'
    )
    parser.add_argument(
        '-o', '--output',
        help='Output file for generated IR'
    )
    parser.add_argument(
        '--no-optimize',
        action='store_true',
        help='Disable optimizations'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Verbose output'
    )
    
    args = parser.parse_args()
    
    # Create compiler
    compiler = MatrixCompiler()
    
    # Compile the file
    module = compiler.compile_file(
        Path(args.input),
        Path(args.output) if args.output else None,
        optimize=not args.no_optimize
    )
    
    print("\nCompilation successful!")

if __name__ == "__main__":
    main()
\end{lstlisting}

\subsection{Part 7: Testing and Verification}

Testing is crucial for compiler correctness. The implementation should include:

\textbf{Test Strategy:}
\begin{itemize}
    \item Unit tests for each compiler phase (parser, dialect, IR generator, optimizer)
    \item Integration tests with complete `.mx` programs
    \item Optimization verification tests showing double transpose elimination
    \item Regression tests for edge cases
\end{itemize}

\textbf{Example Test Output:}
\begin{lstlisting}[language=bash, caption=Running the Compiler]
# Without optimization
$ matrixc --no-optimize examples/main_input.mx
Parsed 2 functions with 10 operations
Generated initial IR
# Shows IR with double transposes: A.T.T, B.T.T, etc.

# With optimization  
$ matrixc examples/main_input.mx
Running optimization passes...
# Shows optimized IR with transposes eliminated
\end{lstlisting}

\subsection{Part 8: Project Structure and Implementation}

The complete implementation is available at: \url{https://github.com/djtodoro/matrix-toy-lang}

\textbf{Project Structure:}
\begin{lstlisting}[caption=Repository Structure]
matrix-toy-lang/
├── src/
│   ├── __init__.py
│   ├── parser.py          # AST-based parser
│   ├── dialect.py         # Matrix MLIR dialect
│   ├── ir_generator.py    # IR generation from AST
│   └── middle_end.py      # Optimization passes
├── examples/
│   └── main_input.mx      # Example input files
├── compiler.py            # Main driver
├── pyproject.toml         # Project configuration
└── README.md             # Documentation
\end{lstlisting}

\textbf{Installation and Usage:}
\begin{lstlisting}[language=bash]
# Setup environment
python -m venv venv
source venv/bin/activate

# Install the compiler
pip install -e .

# Run the compiler
matrixc examples/main_input.mx           # With optimization
matrixc --no-optimize examples/main_input.mx  # Without optimization
\end{lstlisting}

\subsection{Part 9: Lowering to LLVM and Execution}

This crucial step completes your compiler by enabling actual execution of the optimized code on real hardware. You'll lower your custom matrix dialect to LLVM MLIR dialect, then use MLIR tools to generate executable code.

\textbf{Learning Reflection:}

After completing this project, write a short reflection (500 words) addressing:
\begin{itemize}
    \item What was the most challenging part of building the compiler?
    \item How did the optimization change your understanding of compiler design?
    \item What additional optimizations would you implement with more time?
    \item How could this compiler be extended for real-world use?
\end{itemize}

\section{Conclusion: Your Journey Forward}

\subsection{What You've Learned}

Congratulations! You now understand:
\begin{itemize}
    \item How compilers represent programs as structured data (IR)
    \item The role of operations, attributes, and types
    \item How to write analysis passes to understand code
    \item How to implement transformations that optimize programs
    \item The connection between compiler techniques and ML optimization
\end{itemize}

\subsection{Next Steps}

Your compiler journey doesn't end here:

\begin{enumerate}
    \item \textbf{Explore Real Compilers}: Study LLVM, GCC, or V8
    \item \textbf{Contribute to xDSL}: Add new dialects or optimizations
    \item \textbf{Apply to Your Domain}: Build domain-specific compilers
    \item \textbf{Learn Advanced Topics}:
    \begin{itemize}
        \item Polyhedral compilation
        \item Verified compilation
        \item Quantum compilation
        \item Neuromorphic compilation
    \end{itemize}
\end{enumerate}

\subsection{Resources for Further Learning}

\begin{itemize}
    \item \textbf{xDSL Documentation}: \url{https://xdsl.dev}
    \item \textbf{MLIR Tutorial}: \url{https://mlir.llvm.org/docs/Tutorials/}
    \item \textbf{Engineering a Compiler} (Cooper \& Torczon)
    \item \textbf{Modern Compiler Implementation} (Appel)
    \item \textbf{The SSA Book}: \url{http://ssabook.gforge.inria.fr/}
\end{itemize}

\subsection{Final Thoughts}

Compilers are the bridge between human intent and machine execution. As AI/ML systems become more complex and heterogeneous hardware becomes more prevalent, understanding compilation becomes crucial. The techniques you've learned - representation, analysis, and transformation - are fundamental to building efficient, scalable systems.

Remember: Every optimization you write makes programs faster, every analysis you implement makes systems more reliable, and every transformation you create pushes the boundaries of what's computationally possible.

Happy compiling!

\appendix

\section{Appendix A: xDSL Quick Reference}

\subsection{Common Operations}

\begin{lstlisting}[language=Python, caption=xDSL Operations Quick Reference]
# Arithmetic
arith.Constant.from_int_and_width(42, 32)  # Integer constant
arith.Constant.from_float(3.14, f32)       # Float constant
arith.Addi(a, b)  # Integer addition
arith.Muli(a, b)  # Integer multiplication
arith.Addf(x, y)  # Float addition
arith.Mulf(x, y)  # Float multiplication

# Memory
memref.Alloc.get(shape=[10, 20], element_type=f32)
memref.Load(mem, indices)
memref.Store(value, mem, indices)

# Control Flow
scf.For(start, end, step, init_values)
scf.If(condition, has_else=True)
scf.While(init_values)

# Functions
func.FuncOp(name, func_type, body)
func.Call(func_name, args)
func.Return(values)
\end{lstlisting}

\subsection{Common Patterns}

\begin{lstlisting}[language=Python, caption=Common xDSL Patterns]
# Walking operations
for op in module.walk():
    process(op)

# Pattern matching
if isinstance(op, arith.Addi):
    handle_add(op)

# Building IR
with Builder.implicit_region():
    result = arith.Addi(a, b).result

# Replacing operations
old_op.results[0].replace_by(new_value)
old_op.erase()
\end{lstlisting}

\section{Appendix B: Setting Up Your Environment}

\subsection{Installation}

\begin{lstlisting}[language=bash, caption=Installing xDSL]
# Install xDSL
pip install xdsl

# For development
git clone https://github.com/xdslproject/xdsl.git
cd xdsl
pip install -e ".[dev]"

# Run tests
pytest tests/
\end{lstlisting}

\subsection{Your First xDSL Program}

\begin{lstlisting}[language=Python, caption=hello_compiler.py]
#!/usr/bin/env python3

from xdsl.context import Context
from xdsl.dialects import builtin, arith, func
from xdsl.printer import Printer

# Create context and load dialects
ctx = Context()
ctx.load_dialect(builtin.Builtin)
ctx.load_dialect(arith.Arith)
ctx.load_dialect(func.Func)

# Build a simple function
func_type = func.FunctionType.from_lists([builtin.i32], [builtin.i32])

with func.FuncOp("double", func_type).body as func_body:
    arg = func_body.block.args[0]
    two = arith.Constant.from_int_and_width(2, 32)
    result = arith.Muli(arg, two)
    func.Return(result)

# Create module
module = builtin.ModuleOp([func_body.owner])

# Print the IR
printer = Printer()
printer.print_op(module)
\end{lstlisting}

\section{Appendix C: Glossary}

\begin{description}
    \item[AST] Abstract Syntax Tree - Tree representation of source code structure
    \item[Basic Block] Sequence of operations with single entry and exit
    \item[CFG] Control Flow Graph - Graph showing possible execution paths
    \item[Dataflow] Analysis tracking how data moves through program
    \item[Dialect] Collection of related operations and types in xDSL
    \item[Dominance] Relationship where one operation must execute before another
    \item[IR] Intermediate Representation - Compiler's internal program representation
    \item[Lowering] Transforming high-level operations to simpler ones
    \item[Operation] Fundamental unit of computation in xDSL
    \item[Operand] Input value to an operation
    \item[Pass] Transformation or analysis applied to IR
    \item[Pattern] Template for matching and replacing IR structures
    \item[Region] Container for blocks in xDSL (e.g., function body, loop body)
    \item[Result] Output value from an operation
    \item[SSA] Single Static Assignment - Form where each variable assigned once
    \item[Type] Description of data format and constraints
    \item[Use-Def Chain] Links between value definitions and uses
    \item[Value] Data flowing between operations (operands and results)
    \item[Verification] Checking that IR is well-formed and valid
\end{description}

\end{document}
